pytorch 多gpu训练训练:
1. nn.DataParallel: 单机多卡, 在主卡上计算梯度和更新参数, 然后传递给其他gpu, 主卡负载很高..

一行代码即可实现:
model = nn.DataParallel(model)
model.to(device)



2. nn.DistributedDataParallel: 支持单机多卡 和 多机多卡 
每个gpu负责一个进程, 进程内均计算梯度 更新参数 信息互传 [是比较推荐的多gpu训练方式]
每个gpu(进程)load自己的 min-batch: distributedsampler
distributed.init_process_group 做各个进程之间的通信 

1. torch.distributed.init_process_group 初始化进程组
2. torch.nn.parallel.DistributedDataParallel 创建分布式并行模型
3. DistributedSampler制作dataloader
4. torch.multiprocessing or torch.distributed.launch 方式, 开启多gpu训练

注意: 每个epoch都shuffle data的话, 需:
for epoch in range(start_epoch, n_epochs):
    if is_distributed:
        sampler.set_epoch(epoch)
    train(loader)

具体实现:
1. multiprocessing方式:

# 每个进程run一次train(i, args), i: [0, args.gpus-1]
def train(local_rank, args):
    rank = args.nodes * args.gpus + local_rank # 得到全局rank  
    # 初始化进程组
    dist.init_process_group(                                   
        backend='nccl',                                        
        init_method='env://',                                   
        world_size=args.world_size,                              
        rank=rank)                                                          
    # 为每个进程设置seed
    torch.manual_seed(0) 
    model = model()
    torch.cuda.set_device(local_rank)
    model.cuda(local_rank)
    batch_size = 100
    criterion = nn.CrossEntropyLoss().cuda(local_rank)
    optimizer = torch.optim.SGD(model.parameters(), 1e-4)
    
    # model放入DistributedDataParallel内
    model = nn.parallel.DistributedDataParallel(model,
                                                device_ids=[local_rank])

    # Data loading 
    train_dataset = xxx    
    # 每个进程用DistributedSampler读入min-batch数据
    train_sampler = torch.utils.data.distributed.DistributedSampler(
        train_dataset,
        num_replicas=args.world_size,
        rank=rank)

    train_loader = torch.utils.data.DataLoader(
       dataset=train_dataset,
       batch_size=batch_size,
       shuffle=False,
       num_workers=args.num_workers,
       pin_memory=True,
       sampler=train_sampler)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('-n', '--nodes', default=1,
                        type=int, metavar='N')
    parser.add_argument('-g', '--gpus', default=1, type=int,
                        help='number of gpus per node')
    parser.add_argument('-nr', '--nr', default=0, type=int,
                        help='ranking within the nodes')
    parser.add_argument('--epochs', default=2, type=int, 
                        metavar='N',
                        help='number of total epochs to run’)



2. torch.distributed.launch方式实现 [main的写法部分不同, 其他步骤都是line: 15 16 17 一样的.]
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(‘—-local_rank’, type=int, default=0)
    # 初始化进程组, 但init_process_group中无需指定任何参数
    dist.init_process_group(backend='nccl')                                                          
    
    world_size = torch.distributed.get_world_size()
    # 每个进程设置seed
    torch.manual_seed(0)
    model = model()
    torch.cuda.set_device(args.local_rank)
    device = torch.device("cuda", args.local_rank)
    model.cuda(args.local_rank)
    batch_size = 100
    criterion = nn.CrossEntropyLoss().cuda(gpu)
    optimizer = torch.optim.SGD(model.parameters(), 1e-4)
    
    # model放入DistributedDataParallel内
    model = nn.parallel.DistributedDataParallel(model,
                                              device_ids[args.local_rank])

    # Data loading 
    train_dataset = xxx    
    train_sampler = torch.utils.data.distributed.DistributedSampler(
        train_dataset, shuffle=True)

    train_loader = torch.utils.data.DataLoader(
       dataset=train_dataset,
       batch_size=batch_size,
       num_workers=args.num_workers,
       pin_memory=True,
       sampler=train_sampler)

    model.train()
    for i in range(1, EPOCHS + 1):
        train_loader.sampler.set_epoch(i)
        ...

## init_process_group中的参数写在了执行python时候的命令行. [这是比较常见的写法了.. mmdetection就是这种.~]
# python -m torch.distributed.launch --nproc_per_node=2 main.py （2GPUS）
# python -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=0 --master_addr="xx.xx.xx.xx" \
    --master_port=xxxxx main.py （2node 4GPUS）  

